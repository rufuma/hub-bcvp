## 深度学习的基本原理

### 一、线性代数-标量、向量

1. 标量 Scalar 一个单独的数
2. 向量 Vector 一个向量是一列数，空间中的点，美国原生是不同坐标轴的坐标，向量有几个数就是几维向量
   1. 加和：[1,2]+[3,4]=[4,6] 维度相同
   2. 内积：[1,2] * [3,4]=1 * 3+2 * 4=11
   3. 向量夹角余弦值：cosθ = A * B / |A| · |B| , 内积/模长相乘 ,向量的模 |A| = √x₁²+x₂²+...+x²  平方和 开根号。值为空间向量夹角，小则相似

### 二、矩阵 matrix  

矩阵中每个值都是标量，可通过行号和列号进行搜索

1. ```matlab
   [1 2]  2x2的矩阵      	 [1 2]   3*2的矩阵
   [3 4]			       [3 4]
   			 	       [3 4]
   ```

2. ```scala
   [1 2]  +    [1 2]  = [2 4]
   [3 4]		[3 4]    [6 8]		 	    
   ```

3. 乘法 不满足交换律 A * B != B*A

4. 左矩阵A的列等于右矩阵B的行，才可以相乘，M * N 矩阵 乘以 N * P 的矩阵，得到 M * P的矩阵。

   ![image-20250910223730807](C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20250910223730807.png)

5. 分配率  A * (B+C) = A*B+A *C

6. 结合律  A * (B * C) = (A * B) * C

7. 点成，两个矩阵形状一致

   ![image-20250910224400435](C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20250910224400435.png)

8. 矩阵转置(行列互转 transpose)

   ![image-20250910224638171](C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20250910224638171.png)

![image-20250910224654529](C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20250910224654529.png)

### 三、张量 tensor

1. 将三个  2 * 2 的矩阵排列在一起，可以称为 3 * 2 * 2的张量，将四个 3 * 2 * 2的矩阵排列在一起，可称为 4 *  3 * 2 * 2 的张量。

2. 神经网络最常见的数据形式，输入输出，中间结果几乎都是张量的形式 

3. 张量转置 transpose 

   ![image-20250911083908904](C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20250911083908904.png)![image-20250911083938636](C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20250911083938636.png)

​            x.shape = 2 x 2 x 2                 x.transpose(1,2)

![image-20250911084537570](C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20250911084537570.png)![image-20250911084545619](C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20250911084545619.png)

​          x.transpose(0, 1)                     x.view(4, 2)      x.shape = 4 x 2 合并

### 四、numpy常用操作

| 类别     | 函数/方法/属性                       | 说明                                                  |
| :------- | :----------------------------------- | :---------------------------------------------------- |
| 属性     | .dim                                 | 秩，即轴的数量或维度的数量                            |
|          | .shape                               | 对象的尺度，对于矩阵来说，即n行m列                    |
|          | .size                                | 对象的个数，即n*m的值                                 |
|          | .dtype                               | 对象的类型                                            |
| 创建函数 | np.arange(n)                         | 类似range函数，返回ndarray类型，元素从0到n-1          |
|          | np.ones(shape)                       | 根据shape生成一个全1数组，shape是元组类型             |
|          | np.zeros(shape)                      | 根据shape生成一个全0数组                              |
|          | np.full(shape,val)                   | 根据shape生成一个数组，每个元素值都为val              |
|          | np.eye(n)                            | 创建一个正方的n*n单位矩阵，对角线全为1其余为0         |
|          | np.linspace(s,e,n)                   | 根据起始值等间距的填充数据，形成数组                  |
| 数学函数 | np.abs(x), np.fabs(x)                | 计算数组各元素的绝对值                                |
|          | np.sqrt(x)                           | 计算数组各元素的平方根                                |
|          | np.square(x)                         | 计算数组各元素的平方                                  |
|          | np.log(x), np.log10(x), np.log2(x)   | 计算各元素的自然对数、10底对数和2底对数               |
|          | np.ceil(x), np.floor(x)              | 前者向上取整，后者向下取整                            |
|          | np.rint(x)                           | 四舍五入取整                                          |
|          | np.modf(x)                           | 将各元素的小数和整数部分以两个独立的数组形式返回      |
|          | np.cos(x), np.cosh(x)                | 计算各元素的余弦和双曲余弦                            |
|          | np.sin(x), np.sinh(x)                | 计算各元素的正弦和双曲正弦                            |
|          | np.tan(x), np.tanh(x)                | 计算各元素的正切和双曲正切                            |
|          | np.exp(x)                            | 计算各元素的指数值                                    |
|          | np.sign(x)                           | 计算各元素的符号值,1(+),0(0),-1(-)                    |
| 统计函数 | np.sum(a,axis=None)                  | 根据给定轴axis计算数组a相关元素之和，axis为整数或元组 |
|          | np.mean(a,axis=None)                 | 计算期望值                                            |
|          | np.average(a,axis=None,weights=None) | 计算加权平均值                                        |
|          | np.std(a,axis=None)                  | 计算标准差                                            |
|          | np.var(a,axis=None)                  | 计算方差                                              |
|          | np.min(a), np.max(a)                 | 计算a中的最小值，最大值                               |
|          | np.ptp(a)                            | 计算最大值与最小值的差                                |
|          | np.median(a)                         | 计算中位数                                            |
| 数组方法 | .reshape(shape)                      | 不改变数组元素，返回一个shape形状的数组，但原数组不变 |
|          | .resize(shape)                       | 与.reshape()功能一致，但修改原数组                    |
|          | .swapaxes(ax1, ax2)                  | 将数组n个维度中两个进行交换                           |
|          | .flatten()                           | 对数组进行降维,返回折叠后的一维数组，原数组不变       |
| 随机函数 | np.random.rand(d0,d1...,dn)          | 根据d0-dn创建随机数数组，浮点数，[0,1)，均匀分布      |
|          | np.random.randn(d0,d1...,dn)         | 根据d0-dn创建随机数组，标准正态分布                   |
|          | np.random.randint(low,high,shape)    | 根据shape创建随机整数或数组，范围是(low,high)         |
|          | np.random.seed(s)                    | 随机数种子，s是给定的种子值。作用：使得随机数据可预测 |

### 五、导数（函数变化的方向）

1. ![image-20250911085647717](C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20250911085647717.png)

![image-20250911085709950](C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20250911085709950.png)

2. 常见导数

   ![image-20250911085946588](C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20250911085946588.png)![image-20250911090010390](C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20250911090010390.png)

3. 求导法则

   ```math
   加法： [f(x)+g(x)]'=f(x)'+g(x)'
   乘法： [f(x)*g(x)]'=f(x)'*g(x)+g(x)'*f(x)
   除法： [f(x)/g(x)]'=[f(x)'*g(x)-g(x)'*f(x)]/g(x)2
   链式： 若h(x)=f(g(x))，则h'(x)=f'(g(x))g'(x)
   ```

4. 链式求导

   ![image-20250911090758263](C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20250911090758263.png)![image-20250911090813220](C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20250911090813220.png)

   ![image-20250911090722902](C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20250911090722902.png)![image-20250911090822683](C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20250911090822683.png)

### 六、梯度下降

1. 找极小值

   1. 函数f(x)的值受x影响，找到合适的x值，使得f(x)最小
   2. 方法：
      1.任取一点x0，计算在这一点的导数值f(x0)
      2.根据导数的正负，决定x0应当调大还是调小
      3.迭代进行1,2直到x不在变化（或变化极小）

   ![image-20250911112823563](C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20250911112823563.png)

   

   3. 例子：原函数为 y = x²   ，导函数为 y = 2*x，
      1. 在x = -1这个点，导数值为 -2
         该点导数为负数，说明在这一点，如果x增大，y会减小。所以f(x)最小值的点应当在-1的右侧（大于-1）
      2. 在x = 1这个点，导数值为 2
         该点导数为正数，说明在这一点，如果x增大，y会增大。所以f(x)最小值的点应当在1的左侧（小于1）
      3. ![image-20250911113550061](C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20250911113550061.png)

2. 梯度 多元函数的导数，梯度是个向量

   1. 原函数：y = 3x₁² + 4x₂² + 5x₃²   ，导数：y` = 6x₁ + 8x₂ + 10x₃ 。 在[1，1，1]处的梯度是[6，8，10] 

3. 梯度下降 

   1. 根据梯度更新权重；学习率控制梯度更新的幅度；
   2. ![image-20250912085151292](C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20250912085151292.png)
   3. 假设学习率0.1，权重为1，y=x²  梯度为：2x，x=1时候，权重=1-0.1 * 2 = 0.8

4. 求解目标

   1. 损失函数越小，模型越好
   2. 学习的模板是使得损失函数最小
   3. 模型权重影响损失函数值
   4. 通过梯度下降来找到最优权重值，权重更新方式：所有样本一起计算梯度（累加），每次使用一个样本计算梯度，每次使用n个样本计算梯度（累计）

![image-20250912084921362](C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20250912084921362.png)![image-20250912085113125](C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20250912085113125.png)

### 七、全连接神经网络部件

1. 全连接层，又叫做线性层

   1. 公式：A  b 是参与训练的参数，也称为（w   b ), A的维度决定了输出的维度  x输入向量 一般为 1 * D<sub>in</sub>，W权重矩阵，b偏执向量 用于调整输出的线性变换，

   2. $$
      \mathbf{y} = \mathbf{x}\mathbf{W}^T + \mathbf{b}
      $$

   3. 举例：输入：x  维度（1 * 3），A.shape = 5 * 3 ,  b.shape = 1 * 5 ，  X*A ** T =  1 * 5 ， y = 1 * 5 

   4. 权重矩阵 W：

      1. 在Pytorch中 nn.Linear 模块，W的维度是 D<sub>out</sub> * D<sub>in</sub> ，D<sub>in</sub> 是输入特征的数量，D<sub>out</sub> 输出特征的数量
      2. 每个元素 W<sub>ij</sub> 表示输入特征 X<sub>j</sub> 对 输出特征 Y<sub>i</sub>的影响程度
      3. 运算过程过程中需要将其转置 维度为（D<sub>out</sub> * D<sub>in</sub>），来与向量X 相乘
      4. 在训练过程中，w是通过反向传播方式更新的，目标是使得损失函数最小，使得模型预测值尽可能接近真实值

   5. 偏置向量  b：

      1. 维度是D<sub>out</sub>，每个元数b<sub>i</sub> 用于调整 y<sub>i</sub>的线性变换，确保模型具备更好的拟合能力
      2. 可以帮助模型更好地适配数据分布，尤其是输入特征为0，偏置项可以提供非0输出

2. 反向传播

   1. 根据输入x和当前权重 w ，计算预测值y`
   2. 根据y'和y 计算损失函数loss
   3. 根据loss 计算模型权重的梯度
   4. 使用梯度和学习率，根据优化器调整权重w，权重更新方式：所有样本一起计算，每个样本计算，每份n个样本计算（累加）

3. 激活函数-Sigmoid

   1. 为模型添加非线性因素，使得模型具备拟合非线性函数能力 

   2. $$
      f(x) = \frac{1}{1+e^{-x}}
      $$

   3. ![image-20250915143224077](C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20250915143224077.png)

   4. Sigmoid函数是单调增函数，输出范围在[0,1]之间，且越是负向增大，越接近于0，逼近速度越来越慢；越是正向增大，越接近于1，逼近速度也是越来越慢

   5. 缺点：

      1.  Sigmoid函数会造成梯度消失问题，从图像中我们也可以得知，当输入特别大或是特别小时，神经元的梯度几乎接近于0，这就导致神经网络不收敛，模型的参数不会更新，训练过程将变得非常困难。
      2.  Sigmoid函数的输出不是以0为均值的，导致传入下一层神经网络的输入是非0的。这就导致一个后果：若 Sigmoid函数的输出全部为正数，那么传入下一层神经网络的值永远大于0，这时参数无论怎么更新梯度都为正。正是基于上述的缺点， Sigmoid函数近年来的使用频率也在渐渐减弱 

4. 双曲正切激活函数 Tanh

   1. Tanh函数又名双曲正切激活函数，是 Sigmoid函数的变形

   2. $$
      tanh(x) = \frac{sinh(x)}{cosh(x)} = \frac{e^x - e^{-x}}{e^x + e^{-x}}
      $$

   3. tanh激活函数与 Sigmoid函数不同的是，函数的输出范围在[-1,1]之间，且Tanh函数的输出是以为0均值的，这就一定程度上解决了上述 Sigmoid函数的第二个缺点，所以其在实际应用中的效果要好于 Sigmoid函数。但当输入特别大或是特别小时，仍然存在梯度消失的问题

   4. ![image-20250915162507324](C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20250915162507324.png)

5. 修正线性单元 ReLU

   1. 公式f(x)=max(0,x)
   2. ![image-20250915163114415](C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20250915163114415.png)
   3. 从ReLU的函数图像我们可以发现，函数原点左侧的部分，输出值为0，斜率为0；函数原点右侧是斜率为1的直线，且输出值就是输入值。相比于上述的 Sigmoid和tanh两种激活函数，ReLU激活函数完美的解决了梯度消失的问题，因为它的线性的、非饱和的。此外，它的计算也更加简单，只需要设置一个特定的阈值就可以计算激活值，这样极大的提高了运算的速度
   4. 缺点：训练的时候不适合大梯度的输入数据，因为在参数更新之后，ReLU的神经元不会再任何数据节点被激活，这就会导致梯度永远为0。比如：输入的数据小于0时，梯度就会为0，这就导致了负的梯度被置0，而且今后也可能不会被任何数据所激活，也就是说ReLU的神经元“坏死”了。

6. 带泄露的ReLU（Leaky ReLU）和带参数的ReLU（Parametric ReLU）

   1. Leaky ReLU是ReLU激活函数的变形，主要是针对ReLU函数中神经元容易坏死的缺陷，将原点左侧小于0的部分，赋予一个很小的斜率。其效果通常要好于ReLU激活函数，但实践中使用的频率并没有那么高。数据公式为：f(x) = max(0, x) + γmin(0, x)。通常，γ是一个很小的常数，如：0.01。
   2. Parametric ReLU是ReLU激活函数的另一种变形，和Leaky ReLU函数一样是非饱和函数，解决了坏死难题。不同之处在于其在函数中引入一个可学习的参数，往往不同的神经元有不同的参数，所以第i个神经元的数学表达式为：f(x) = max(0, x) + γi min(0, x)。当γi 取0时，便可以视为ReLU函数，取很小的常数时，可视为Leaky ReLU函数
   3. ![image-20250915163723345](C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20250915163723345.png)

7. GELU 激活函数

   1. GELU (Gaussian Error Linear Units) 是一种基于高斯误差函数的激活函数，相较于 ReLU 等激活函数，GELU 更加平滑，有助于提高训练过程的收敛速度和性能。

   2. $$
      \text{GELU}(x) = 0.5x \left(1 + \tanh\left(\sqrt{\frac{2}{\pi}} \left(x + 0.044715x^3\right)\right)\right)
      $$

   3. 其中 2/π  和 0.044715  GELU 函数的两个调整系数

   4. ![image-20250915185040638](C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20250915185040638.png)

   5. 特性：

      1. 非线性： GELU引入了非线性变换，使得神经网络能够学习更复杂的映射，有助于提高模型的表达能力
      2. **平滑性：** GELU是一个平滑的函数，具有连续的导数，没有梯度截断的问题，有助于梯度的稳定传播。并且GELU 在 0 附近比 ReLU 更加平滑，因此在训练过程中**更容易收敛**。
      3. **防止神经元“死亡”：** 不同于ReLU在负数范围内完全置零，GELU在负数范围内引入了一个**平滑的非线性**，有助于防止神经元“死亡”问题。
      4. **高斯分布：** GELU激活函数的输出在输入接近于 0 时接近于高斯分布，这有助于**提高神经网络的泛化能力**，使得模型更容易适应不同的数据分布。
      5. **计算资源效率：** GELU激活函数的**计算相对复杂**，涉及到指数、平方根和双曲正切等运算，因此在计算资源有限的情况下可能会带来较大的计算开销。
      6. **趋向于线性**：对于较大的输入值，GELU函数的输出**趋向于线性**，可能会导致一些非线性特征的丢失。
      7. 相对于 Sigmoid 和 Tanh 激活函数，ReLU 和 GeLU 更为准确和高效，因为它们在神经网络中的梯度消失问题上表现更好。梯度消失通常发生在深层神经网络中，意味着梯度的值在反向传播过程中逐渐变小，导致网络梯度无法更新，从而影响网络的训练效果。而 ReLU 和 GeLU 几乎没有梯度消失的现象，可以更好地支持深层神经网络的训练和优化。

   6. 损失函数-均方差

      1. $$
         L(y, \hat{y}) = \frac{1}{n}\sum (y_i - \hat{y}_i)^2
         $$

      2. y<sub>i</sub> 是第 i 个样本的真实值；y' 是第 i 个样本的预测值； n 样本数量

      3. ![image-20250915192723349](C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20250915192723349.png)

      4. ```pyt
         import torch
         import torch.nn as nn
         
         # 内置实现
         criterion = nn.MSELoss()
         loss = criterion(y_pred, y_true)
         
         # 手动实现
         def mse_loss_torch(y_pred, y_true):
             return torch.mean((y_pred - y_true) ** 2)
         ```

      5. 优点：

         1. **可微性**：处处可导，便于梯度下降优化
         2. **凸函数**：对于线性模型，保证找到全局最优解
         3. **计算高效**：计算简单，时间复杂度为 O(n)
         4. **物理意义明确**：直接反映预测值与真实值的欧氏距离

      6. 缺点：

         1. **对异常值敏感**：平方项会放大异常值的影响
         2. **可能梯度爆炸**：当误差很大时，梯度可能过大
         3. **不是概率解释**：不如交叉熵损失函数适用于分类问题

      7. 应用场景：

         1. **回归问题**：房价预测、温度预测等连续值预测
         2. **图像重建**：自编码器、图像超分辨率重建
         3. **生成模型**：GANs中的判别器损失（有时）
         4. **强化学习**：值函数近似

      8. 变种：

         1. **加权MSE**：为不同样本分配不同权重

         2. $$
            L = \frac{1}{n}\sum_{i=1}^{n} w_i(y_i - \hat{y}_i)^2
            $$

         3. **RMSE**（均方根误差）

            1. $$
               L = \sqrt { \frac{1}{n}\sum_{i=1}^{n} w_i(y_i - \hat{y}_i)^2 }
               $$

            2. **量纲一致性**：与目标值具有相同的量纲，便于解释；**放大大误差**：对大误差更加敏感，强调严重预测错误；**可微性**：在正值区域可导，便于优化

            3. **对异常值敏感**：平方根操作仍受大误差影响；**非鲁棒性**：异常值会显著影响RMSE值

            4. 场景：需要误差与目标值同量纲的解释；强调大误差的惩罚（如金融风险预测）；模型性能比较时需要直观的误差度量

            5. 选择指南：需要误差量与目标值同量纲；想要强调和惩罚大误差；数据中异常值较少或需要检测异常值；用于模型比较和基准测试

         4. **MAE**(Mean Absolute Error) - 平均绝对误差

            1. $$
               L = \frac{1}{n}\sum_{i=1}^{n} w_i|y_i - \hat{y}_i|
               $$

            2. **鲁棒性强**：对异常值不敏感，更加稳定；**直观解释**：平均绝对误差，易于理解；**线性惩罚**：误差与惩罚成线性关系

            3. **在零点不可导**：不利于梯度下降优化；**可能收敛慢**：梯度恒定，收敛速度可能较慢；**不强调大误差**：所有误差同等对待

            4. 场景：数据中存在异常值时；需要鲁棒的误差度量（如传感器数据）；误差分布可能具有重尾时；

            5. 选择指南：数据中存在异常值或噪声；需要鲁棒的误差度量；所有误差应该被平等对待；计算效率很重要

   7. 损失函数-交叉熵

      1. 二分交叉熵

      2. $$
         L(y, \hat{y}) = -\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \cdot \log(\hat{y}_i) + (1 - y_i) \cdot \log(1 - \hat{y}_i) \right]
         $$

      3. 多分类交叉熵（Softmax交叉熵）
         $$
         L(y,\hat{y}) = -\frac{1}{N} \sum_{i=1}^{N} \sum_{c=1}^{C} y_{i,c} \cdot \log(\hat{y}_{i,c})
         $$

      4. 内容：N: 样本数量；C: 类别数量；y_i: 真实标签（one-hot编码或类别索引）；y ’ :  预测概率分布；y_{i,c}: 第i个样本属于类别c的真实概率（通常为0或1）；y ^ : 第i个样本属于类别c的预测概率

      5. 优点： 

         1. **概率解释性强**：直接优化预测概率与真实分布的差异
         2. **梯度性质好**：提供平滑的梯度，有利于优化
         3. **理论基础扎实**：基于信息论，有很好的数学解释
         4. **类别不平衡处理**：可以通过加权处理不平衡数据
         5. **数值稳定性**：现代框架实现了数值稳定的版本

      6. 缺点：

         1. **对错误标注敏感**：噪声标签会显著影响训练
         2. **可能过拟合**：特别是当模型复杂而数据少时
         3. **需要概率输出**：要求模型输出概率分布
         4. **计算复杂度**：相比MSE等回归损失计算量稍大

      7. 示例  ： 文本分类模型

         ```python
         # 文本分类模型
         class TextClassifier(nn.Module):
             def __init__(self, vocab_size, embed_dim, num_classes):
                 super().__init__()
                 self.embedding = nn.Embedding(vocab_size, embed_dim)
                 self.lstm = nn.LSTM(embed_dim, 128, batch_first=True)
                 self.fc = nn.Linear(128, num_classes)
             
             def forward(self, x):
                 x = self.embedding(x)
                 _, (hidden, _) = self.lstm(x)
                 return self.fc(hidden[-1])
         
         criterion = nn.CrossEntropyLoss()
         ```

         

      8. 作业：求导，计算损失函数得到最小值

         ```python
         from matplotlib import pyplot
         
         X = [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35000000000000003, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41000000000000003, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47000000000000003, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.5700000000000001, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.6900000000000001, 0.7000000000000001, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.8200000000000001, 0.8300000000000001, 0.84, 0.85,
         0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.9400000000000001, 0.9500000000000001, 0.96, 0.97, 0.98, 0.99]
         Y = [4.0, 4.0302, 4.0608, 4.0918, 4.1232, 4.155, 4.1872, 4.2198, 4.2528, 4.2862, 4.32, 4.3542, 4.3888, 4.4238, 4.4592, 4.495, 4.5312, 4.5678, 4.6048, 4.6422, 4.68, 4.7181999999999995, 4.7568, 4.7958, 4.8352, 4.875, 4.9152000000000005, 4.9558, 4.9968, 5.0382, 5.08, 5.122199999999999, 5.1648, 5.2078, 5.2512, 5.295, 5.3392, 5.3838, 5.4288, 5.4742, 5.5200000000000005, 5.5662, 5.6128, 5.6598, 5.7072, 5.755, 5.8032, 5.851800000000001, 5.9008, 5.9502, 6.0, 6.0502, 6.1008, 6.1518, 6.203200000000001, 6.255000000000001, 6.3072, 6.3598, 6.4128, 6.4662, 6.52, 6.5742, 6.6288, 6.6838, 6.7392, 6.795, 6.8512, 6.9078, 6.9648, 7.022200000000001, 7.08, 7.138199999999999, 7.1968, 7.2558, 7.3152, 7.375, 7.4352, 7.4958, 7.5568, 7.6182, 7.680000000000001, 7.7422, 7.8048, 7.867800000000001, 7.9312, 7.994999999999999, 8.0592, 8.1238, 8.1888, 8.2542, 8.32, 8.3862, 8.4528, 8.5198, 8.587200000000001, 8.655000000000001, 8.7232, 8.7918, 8.8608, 8.9302]
         
         w1, w2, w3 = 0, 0, 0
         
         """
         求导，计算损失函数得到最小值
         """
         
         # 定义函数
         def func(x):
             return w1 * x ** 2 + w2 * x + w3
         
         def loss(y_pred, y_true):
             return (y_pred - y_true) ** 2
         
         lr = 0.1
         
         batch_size = 10
         
         for epoch in range(1000):
             grad_w1 = 0
             grad_w2 = 0
             grad_w3 = 0
             count = 0
             epoch_loss = 0
             for x, y in zip(X, Y):
                 count += 1
                 y_pred = func(x)
                 epoch_loss += loss(y, y_pred)
                 grad_w1 += 2 * (y_pred - y) * x ** 2
                 grad_w2 += 2 * (y_pred - y) * x
                 grad_w3 += 2 * (y_pred - y)
                 if count == batch_size:
                     w1 = w1 - lr * grad_w1 / count
                     w2 = w2 - lr * grad_w2 / count
                     w3 = w3 - lr * grad_w3 / count
                     grad_w1 = 0
                     grad_w2 = 0
                     grad_w3 = 0
                     count = 0
             epoch_loss = epoch_loss / len(X)
             print('Epoch {} Loss {}'.format(epoch, epoch_loss))
             if epoch_loss < 0.0001:
                 break
         
         print(f"训练后权重：w1:{w1} w2:{w2} ,w3:{w3}")
         ```

   8. 二分类任务     

   ```python
   import numpy as np
   import torch.nn
   from torch import nn
   
   """
   基于pytorch框架编写模型训练
   实现一个自行构造的找规律(机器学习)任务
   规律：x是一个5维向量，如果第1个数>第5个数，则为正样本，反之为负样本
   """
   
   class TorchModel(nn.Module):
       # 初始化
       def __init__(self , input_size, hidden_size, output_size):
           super(TorchModel, self).__init__()
           # 线性层
           self.linear = nn.Linear(input_size, hidden_size)
           # GELU激活函数
           self.gelu = nn.GELU()
           # 线性层
           self.linear2 = nn.Linear(hidden_size, output_size)
           # 交叉熵损失函数 包含softmax
           self.loss = nn.CrossEntropyLoss()
   
       # 定义模型
       def forward(self, x, y=None):
           x = self.linear(x)
           x = self.gelu(x)
           y_pre = self.linear2(x)
           if y is None:
               return y_pre
           else:
               if y.dim() > 1:
                   # 移除张量中所有维度为1的维度（压缩维度） 将 [[1], [0], [1], [0]] 变成 [1, 0, 1, 0]
                   y = y.squeeze()
               if y.dtype != torch.long:
                   # 将张量的数据类型转换为长整型（int64） 将 [1.0, 0.0, 1.0, 0.0] 变成 [1, 0, 1, 0]
                   y = y.long()
               return self.loss(y_pre, y)
   
   
   # 生成样本，如果第一个数值大于第五个返回1，否则返回0
   def build_sample():
       # x = torch.randn(1, 5)
       x = np.random.random(5)
       if x[0] > x[4]:
           return x, 1  # 正样本
       else:
           return x, 0  # 负样本
   
   
   # 数据集准备
   def build_dataSet(train_total):
       X = []
       Y = []
       for i in range(train_total):
           x, y = build_sample()
           X.append(x)
           Y.append(y)   # 直接存储标量，不是列表
       return torch.FloatTensor(X), torch.FloatTensor(Y)
   
   
   # 模型训练
   def trainModel():
       # 训练轮次
       epochs = 20
       # 训练集总数
       train_total = 5000
       # 每次训练数量
       batch_size = 20
       # 输入
       input_size = 5
       hidden_size = 10
       # 输出 二分类：两个类别
       output_size = 2
   
       # 建立模型
       model = TorchModel(input_size, hidden_size, output_size)
       # 学习率定义
    lr = 0.001
       # 优化器定义
       optim = torch.optim.Adam(model.parameters(), lr=lr)
       # 训练集构建
       train_x, train_y = build_dataSet(train_total)
   
       # 开始训练
       for epoch in range(epochs):
           model.train(True)
           epoch_loss = []
           for batch_index in range(train_total // batch_size):
               x = train_x[batch_index * batch_size:(batch_index + 1) * batch_size]
               y = train_y[batch_index * batch_size:(batch_index + 1) * batch_size]
   
               # 计算损失函数
               loss = model(x, y)
               # 计算梯度
               loss.backward()
               # 更新权重
               optim.step()
               # 梯度归0
               optim.zero_grad()
   
               # 存储 loss
               epoch_loss.append(loss.item())
   
           print("第 {} 轮，平均Loss {:.6f}".format(epoch+1, np.mean(epoch_loss)))   # 保留6位小数：
   
       # 保存模型
       torch.save(model.state_dict(), '../model/week02/binary_classification_model.bin')
       # 画图
       return model
   
   # 验证模型
   def predict(model_path, input_vec):
       input_size = 5
       hidden_size = 10
       output_size = 2
       # 构建模型
       model = TorchModel(input_size, hidden_size, output_size)
       # 加载模型
       model.load_state_dict(torch.load(model_path))
       # 测试模型
       model.eval()
   
       # 不计算梯度
       with torch.no_grad():
           # 原始输出
           logits = model.forward(torch.FloatTensor(input_vec))
           # 使用softmax 获取概率 每个类别的可能性是多少  将模型的原始输出转换为概率分布  把 [2.0, 1.0] 这样的原始分数变成 [0.7311, 0.2689] 这样的概率值，所有类别概率加起来等于1。
           probabilities = torch.softmax(logits, dim=1)  # 给人看的，显示每个类别的置信度
           # 获取预测类别,从概率中选出最可能的类别  找出每个样本中概率最大的类别索引  argmax是根据索引位置来决定类别的，  样本1：[0.7311, 0.2689] → 类别0（因为0.7311 > 0.2689） 样本2：[0.1824, 0.8176] → 类别1（因为0.8176 > 0.1824）
           predictions = torch.argmax(logits, dim=1)  # 给程序用的，直接得到最终预测结果
   
       for inp, prob, pred in zip(input_vec, probabilities, predictions):
           # 真实值
           actual_class = 1 if inp[0] > inp[4] else 0  # python 三元表达式
           correct_class = "✓" if pred == actual_class else "✗"
           print(f"输入：{[f'{val:.6f}' for val in inp]},"
                 f"实际：{actual_class},预测：{pred},"   # pred.item() 从张量中提取出预测的数值
                 f"概率：[0:{prob[0]:.4f},{prob[1]:.4f}] {correct_class}")
   
   
   # 验证数据
   if __name__ == '__main__':
       # 训练
       trainModel()
       # 构建验证集
       test_vec = [[0.97889086,0.15229675,0.31082123,0.03504317,0.88920843],
                   [0.74963533,0.5524256,0.95758807,0.95520434,0.84890681],
                   [0.00797868,0.67482528,0.13625847,0.34675372,0.19871392],
                   [0.99349776,0.59416669,0.92579291,0.41567412,0.1358894]]
       # 验证
       predict('../model/week02/binary_classification_model.bin', test_vec)
   
   
   输入：['0.978891', '0.152297', '0.310821', '0.035043', '0.889208'],实际：1,预测：1,概率：[0:0.0511,0.9489] ✓
   输入：['0.749635', '0.552426', '0.957588', '0.955204', '0.848907'],实际：0,预测：0,概率：[0:0.9353,0.0647] ✓
   输入：['0.007979', '0.674825', '0.136258', '0.346754', '0.198714'],实际：0,预测：0,概率：[0:0.9942,0.0058] ✓
   输入：['0.993498', '0.594167', '0.925793', '0.415674', '0.135889'],实际：1,预测：1,概率：[0:0.0000,1.0000] ✓
   ```


   ​      

   ### 八、作业

   改用交叉熵实现一个多分类任务，五维随机向量最大的数字在哪维就属于哪一类

   ```python
   import torch
   from torch import nn
   
   
   class LargestNumCrossEntropyLoss(nn.Module):
   
       def __init__(self, hidden_size):
           super(LargestNumCrossEntropyLoss, self).__init__()
           # 线性层
           self.linear1 = nn.Linear(5, hidden_size)
           # 激活函数GELU
           self.gelu = nn.GELU()
           # 天津dropout 防止过拟合
           self.dropout = nn.Dropout(p=0.2)
           # 输出
           self.linear2 = nn.Linear(hidden_size, 5)
   
       def forward(self, x):
           x = self.gelu(self.linear1(x))
           x = self.dropout(x)
           x = self.linear2(x)
           return x
   
   def train_model():
       # 生成数据 创建了一个形状为 (1000, 5) 的张量，其中的元素是从标准正态分布（均值为0，标准差为1）中随机抽取的
       x = torch.randn(100, 5)
       # 它沿着指定维度找到最大值的位置（索引）   dim=1 表示：沿着行的方向，对每一行进行操作。
       y = torch.argmax(x, dim=1)
   
       # 模型，调用的以上forward方法
       model = LargestNumCrossEntropyLoss(hidden_size=100)
       # 损失函数（内部会自动做softmax）
       criterion = nn.CrossEntropyLoss()
       # 模型优化器  学习率太小会导致训练过慢，太大会导致无法收敛
       optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
       for epoch in range(100):
           # 向前传播
           # 预测值
           y_pred = model(x)
           # 计算损失函数
           loss = criterion(y_pred, y)
           # 反向传播
           # 计算梯度
           loss.backward()
           # 更新权重
           optimizer.step()
           # 梯度归0
           optimizer.zero_grad()
   
           if epoch % 20 == 0:
               print(f'Epoch: {epoch}, Loss: {loss.item():.4f}')
               print(f'x：{x},y_pred: {y_pred}', end='\n\n')
   
       # 保存模型
       torch.save(model.state_dict(), 'model.pt')
       return model
   
   
   def test_model(model_path, x_test):
       # 1. 初始化模型实例  创建与训练时相同结构的模型对象  hidden_size=100 必须与训练时的模型结构保持一致
       model = LargestNumCrossEntropyLoss(hidden_size=100)
       # 2. 加载训练好的模型参数  torch.load() 从指定路径加载模型的状态字典 load_state_dict() 将加载的参数赋给当前模型
       model.load_state_dict(torch.load(model_path))
       # 3. 设置模型为评估模式  这会禁用dropout、batchnorm等训练特有的层 确保测试结果的一致性和可重复性
       model.eval()
   
       # 4.禁用梯度计算，节省内存并加速推理 在测试阶段不需要计算梯度，因为我们不会更新模型参数
       with torch.no_grad():
           #   5. 前向传播，获取模型输出 output 的形状为 [batch_size, 5]，表示每个样本对5个类别的预测分数
           output = model(x_test)
           # 6. 获取预测结果  torch.argmax(dim=1) 沿着第1维度（类别维度）取最大值索引  predicted 的形状为 [batch_size]，包含每个样本的预测类别索引（0-4）
           predicted = torch.argmax(output, dim=1)
           # 7. 计算概率分布 使用softmax将输出分数转换为概率值，所有类别概率之和为1  probabilities 的形状为 [batch_size, 5]
           probabilities = torch.softmax(output, dim=1)
   
           print(f'x_test: {x_test},output:{output}, 预测最大值位置: {predicted}')
           print(f"  实际最大位置：{torch.argmax(x_test, dim=1)},概率分布: {probabilities.numpy()}")
   
   
   if __name__ == '__main__':
       train_model()
       test_vec = torch.randn(3, 5)
       test_model(model_path='model.pt', x_test=test_vec)
   
   ```

   

